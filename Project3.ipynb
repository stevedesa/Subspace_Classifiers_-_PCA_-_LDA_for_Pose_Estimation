{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Understanding data representation through low-dimensional projections (subspace learning)\n",
    "\n",
    "### Name: Steve Nathan de Sa\n",
    "### Course Level: CSC 448\n",
    "\n",
    "### Due: Monday March 24, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "* In this project, we explore the application of classification using: a) Principal Componenet Analysis (PCA) and b) Linear Discriminant Analysis (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "* The objective of this project is to implement different classification models to analyze real-world datasets, understand the relationship between variables, and perform classifications.  Additionally, students will gain experience understanding optimization techniques, linear algebra, and subspace learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first problem we aim to design a pose (orientation) estimator for a set of image data.  Please download the Boat.zip file from the D2L project 3 module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem A (60pts)**\n",
    "\n",
    "1 (5pts). The first thing you'll need to do is read the image data in and construc the image data matrix $X = [\\textbf{x}_1, \\textbf{x}_2, \\cdots, \\textbf{x}_n]$ where each column of $X$ is a \"row-scanned\" image\n",
    "$$\n",
    "    \\textbf{x}_i = \\text{vec}(I_i) \\in \\mathbb{R}^{m}\n",
    "$$\n",
    "where $m$ is the number of pixels in the image (i.e., turn the image into a single column vector - hint: np.reshape works here)\n",
    "\n",
    "For this particular dataset, there are $n=128$ images sampled at a different orientation around a single degree of freedom (i.e, the object being rotated around a single axis through 360 degrees).  Each image $I \\in [0,1]^{h \\times v}$ where $h=v=128$ and $m=16,384$ features.  \n",
    "\n",
    "* This will result in a image data matrix $X \\in \\mathbb{R}^{16,384 \\times 128}$\n",
    "\n",
    "**Note:** The .zip also contains the Testing Set (same object imaged at random angles around the same single degree of freedom) along with a .txt file providing the actual pose (orientation) of each training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data (however you prefer), and Construct the Image data matrix X #\n",
    "train_path = \"Boat/Train/UnProcessed\"\n",
    "test_path = \"Boat/Test/Boat32/UnProcessed\"\n",
    "pose_path = \"Boat/Test\"\n",
    "pose_filename = 'RandAng.txt'\n",
    "\n",
    "# Load training data\n",
    "def load_training_data(train_path):\n",
    "    n_images = 128\n",
    "    img_size = 128 * 128\n",
    "    X_train = np.zeros((img_size, n_images))\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        img_path = os.path.join(train_path, f'img_{i}.png')\n",
    "        img = imread(img_path)\n",
    "        X_train[:, i] = img.reshape(-1)\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "# Load test data\n",
    "def load_test_data(test_path):\n",
    "    n_test = 64\n",
    "    img_size = 128 * 128\n",
    "    X_test = np.zeros((img_size, n_test))\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        img_path = os.path.join(test_path, f'img_{i}.png')\n",
    "        img = imread(img_path)\n",
    "        X_test[:, i] = img.reshape(-1)\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "# Load + Read true poses\n",
    "def read_true_poses(pose_path, pose_filename):\n",
    "    pose_file_path = os.path.join(pose_path, pose_filename)\n",
    "    \n",
    "    with open(pose_file_path, 'r') as f:\n",
    "        content = f.read().strip()\n",
    "        poses_str = content.split(',')\n",
    "        true_poses = [float(pose) for pose in poses_str]\n",
    "    \n",
    "    return np.array(true_poses)\n",
    "\n",
    "# Load matrix data [X]\n",
    "X_train = load_training_data(train_path)\n",
    "X_test = load_test_data(test_path)\n",
    "\n",
    "# Set Info Recovery ratio\n",
    "mu_set = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (5pts). Next, we need to ensure the data is centered, so we subtract the mean image vector $\\bar{\\textbf{x}}$ from each image in the dataset.  Keep this for later classification! Write a function called CenterData that takes the original $X$ and returns the centered $X$ (referred to as $\\bar{X}$) along with the mean vector $\\bar{\\textbf{x}}$ associated with $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the classification rate between models predicted target and true value #\n",
    "def CenterData(d_X):\n",
    "    mean_x = np.mean(d_X, axis=1, keepdims=True)\n",
    "    barX = d_X - mean_x\n",
    "    return barX, mean_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (10pts). We will need a way to \"select\" the optimum subspace dimension (i.e., how many principal components to keep).  One way to do this is to use the information recovery ratio defined as:\n",
    "$$\n",
    "    \\rho(\\bar{X},\\mu) = \\frac{\\sum_{i=1}^k \\sigma_i^2}{\\| \\bar{X} \\|^2_F} \\leq \\mu\n",
    "$$\n",
    "where $\\mu \\in [0,1]$ is the amount of information recovered from a $k$-dimensional subspace (note $\\rho \\rightarrow 1$ as $k \\rightarrow n$) resulting in 100\\% recovery and $\\sigma_i$ is the $i^{\\textbf{th}}$ singular value of $\\bar{X}$.\n",
    "\n",
    "* Write a function called InfoRecov that takes the \\underline{centered} image data matrix $\\bar{X}$, the mean vector $\\bar{\\textbf{x}}$, and user specified information recovery $\\mu$ that returns the value $k$ required to acheive the user-specified information recovery.\n",
    "\n",
    "\n",
    "**Note:** Generally $\\mu=0.85 - 0.9$ is sufficient for good classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Recovery\n",
    "def InfoRecov(barX, mean_x, mu):\n",
    "    U, S, Vt = np.linalg.svd(barX, full_matrices=False) # SVD decomp\n",
    "    \n",
    "    # Calculate cumulative energy ratio\n",
    "    sigma_squared = S**2\n",
    "    frobenius_norm = np.sum(sigma_squared)\n",
    "    cumulative_energy = np.cumsum(sigma_squared) / frobenius_norm\n",
    "    \n",
    "    # Find k that achieves desired information recovery ratio\n",
    "    k_keep = np.argmax(cumulative_energy >= mu) + 1\n",
    "    return k_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 (20pts). Next write a function (called PCA) that takes the image data matrix $X$, and user specified accuracy $\\mu$, to compute and return the first $k$ principal components of $X$ where $k$ is the subspace dimension required to acheive an information recovery $\\mu$.  \n",
    "\n",
    "**Note:** You will need to use the above \"helper\" functions to:\n",
    "\n",
    "1. Ensure the data is centered prior to computation\n",
    "2. Compute how many pricipal components to keep via the user specified information recovery $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logistic regression model parameters #\n",
    "def PCA(data_X, mu):\n",
    "    barX, mean_x = CenterData(data_X) # Center data\n",
    "    \n",
    "    U, S, Vt = np.linalg.svd(barX, full_matrices=False) # Get SVD\n",
    "    \n",
    "    k_keep = InfoRecov(barX, mean_x, mu) # How many comps to keep?\n",
    "    \n",
    "    u_pca = U[:, :k_keep] # Get first k comps\n",
    "    \n",
    "    return u_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 (10pts). Next write a function called DataProjection that takes the first $k$ principal components, the centered data matrix $\\bar{X}$, and computes the change of basis $Y$ embedding the data into a $k$-dimensional subspace.  To illustrate your function is working, generate a $k=3$-dimensional plot showing the low-dimensional embedding in $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Projection\n",
    "def DataProjection(barX, u_pca):\n",
    "    y_embed = u_pca.T @ barX\n",
    "    return y_embed\n",
    "\n",
    "# Center data & plot k=3 for visualization\n",
    "barX_train, mean_x = CenterData(X_train)\n",
    "u_pca_3 = PCA(X_train, mu_set)[:, :3]\n",
    "Y_3 = DataProjection(barX_train, u_pca_3)\n",
    "\n",
    "# Just to make sure its k = 3\n",
    "if u_pca_3.shape[1] == 3:\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(Y_3[0, :], Y_3[1, :], Y_3[2, :])\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.set_title('3D Projection of Training Image Data')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 (10pts). Evaluate how well your low-dimensional projection works by estimating the pose of each image in the Test Set.  You may want to write a quick helper function here but it's not required.  The process for evaluation is as follows:\n",
    "\n",
    "1. Vectorize a test image and subtract the mean computed above $\\bar{\\textbf{x}}$.\n",
    "2. Project the test image onto the low-dimensional subspace (performa a change of basis $\\textbf{t}=U_k^T \\textbf{f}$) where $U_k \\in \\mathbb{R}^{m \\times k}$ contains the first $k$ principal components, $\\textbf{f} \\in \\mathbb{R}^m$ is the mean-centered vectorized image, and $\\textbf{t} \\in \\mathbb{R}^k$ is the low-dimensional projection.\n",
    "3. Perform a nearest-neighbor search with all other samples in $Y$ to compute which sample in $Y$ that $\\textbf{t}$ is closest to.  \n",
    "4. Use the equation $est = \\frac{2 \\pi i}{n}$ to estimate the orientation of the test sample (where $i$ is the closest sample to $\\textbf{t}$)\n",
    "5. Compute the estimation error by looking at the .txt file to determine the \"true\" orientation and subtracting the two.\n",
    "6. Do this for all 64 test samples to determine the average classification error and standard deviation across the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform pose estimation and return the estimation results (mean and standard deviation) #\n",
    "\n",
    "# Pose Estimation\n",
    "def estimate_poses(X_train, X_test, test_path, pose_filename, mu):\n",
    "    barX_train, mean_x = CenterData(X_train) # Center train data\n",
    "    \n",
    "    u_pca = PCA(X_train, mu) # Get principal comps\n",
    "    \n",
    "    Y_train = DataProjection(barX_train, u_pca) # Project to lower dim\n",
    "    \n",
    "    barX_test = X_test - mean_x # Center using mean\n",
    "    \n",
    "    T_test = u_pca.T @ barX_test # Project to lower dim\n",
    "    \n",
    "    # Read true poses\n",
    "    true_poses = read_true_poses(pose_path, pose_filename)\n",
    "    \n",
    "    n_train = X_train.shape[1]\n",
    "    errors = []\n",
    "    \n",
    "    # For each test image\n",
    "    for i in range(X_test.shape[1]):\n",
    "        distances = np.linalg.norm(Y_train - T_test[:, i:i+1], axis=0) # Calc dist to all train samples\n",
    "        \n",
    "        nearest_idx = np.argmin(distances) # Get nearest neighbor\n",
    "        \n",
    "        est_pose = 2 * np.pi * nearest_idx / n_train # Est pose using nearest neighbor's index\n",
    "        \n",
    "        true_pose = true_poses[i] # Get true pose\n",
    "        \n",
    "        # Make sure they are in range 0 -> 2Ï€\n",
    "        est_pose = est_pose % (2 * np.pi)\n",
    "        true_pose = true_pose % (2 * np.pi)\n",
    "        \n",
    "        # Calc smallest circular difference\n",
    "        error = min(abs(est_pose - true_pose), 2 * np.pi - abs(est_pose - true_pose))\n",
    "        errors.append(error)\n",
    "    \n",
    "    errors = np.array(errors)    \n",
    "    mean_error = np.mean(errors)\n",
    "    std_error = np.std(errors)\n",
    "    \n",
    "    return mean_error, std_error, errors\n",
    "\n",
    "# Run pose estimation\n",
    "mean_err, std_err, all_errors = estimate_poses(X_train, X_test, test_path, pose_filename, mu_set)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean estimation error (radians): {mean_err:.4f} (degrees): {np.degrees(mean_err):.2f}\")\n",
    "print(f\"Std Dev of error (radians): {std_err:.4f} (degrees): {np.degrees(std_err):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem B (45pts)**\n",
    "\n",
    "\n",
    "1 (5pts). Next, let's look at a multi-class problem and compute a linear subspace (change of basis) by exploring Linear Discriminant Analysis.  \n",
    "\n",
    "Here, we want to investigate the first three odd digits of the MNIST dataset.  Let's go ahead and get the dataset loaded (60,000 training images and 10,000 testing images)\n",
    "\n",
    "**The data can be downloaded** [Here](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)\n",
    "\n",
    "- Note that once loaded, you will need to \"parse\" the training and testing set to only keep the first three odd numbers (1,3, and 5).\n",
    "- These are also images (size: $28 \\times 28$) so you'll need to perform the same \"vectorization\" of each image into a 784-dimensional vector.\n",
    "\n",
    "\n",
    "Let's also plot a few digits to get the hang of working with the dataset by creating a $3 \\times 3$ subplot to display 9 total digits.\n",
    "\n",
    "**Note:** When displaying images you need to provide a colormap, these are grayscale images so our colormap will be 'gray'\n",
    "\n",
    "* $\\texttt{plt.imshow(I,cmap='gray')}$ should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST data, separte the data into only 1,3 and 5s for both training and testing, plot the first 9 digits #\n",
    "mnist_data_dir = 'archive'\n",
    "\n",
    "# Special extrn fn for imgs\n",
    "def load_idx_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, rows * cols)\n",
    "    return images\n",
    "\n",
    "# Special extrn fn for labels\n",
    "def load_idx_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "# Load MNIST data\n",
    "def load_mnist_data(data_dir):\n",
    "    files = {\n",
    "        'train_images': 'train-images.idx3-ubyte',\n",
    "        'train_labels': 'train-labels.idx1-ubyte',\n",
    "        'test_images': 't10k-images.idx3-ubyte',\n",
    "        'test_labels': 't10k-labels.idx1-ubyte'\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        'train_images': load_idx_images(os.path.join(data_dir, files['train_images'])),\n",
    "        'train_labels': load_idx_labels(os.path.join(data_dir, files['train_labels'])),\n",
    "        'test_images': load_idx_images(os.path.join(data_dir, files['test_images'])),\n",
    "        'test_labels': load_idx_labels(os.path.join(data_dir, files['test_labels']))\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "mnist_data = load_mnist_data(mnist_data_dir)\n",
    "\n",
    "# Extract only 1, 3, 5\n",
    "def extract_1_3_5_digits(images, labels, odd_digits=[1, 3, 5]):\n",
    "    mask = np.isin(labels, odd_digits)\n",
    "    return images[mask], labels[mask]\n",
    "\n",
    "# Extract data\n",
    "X_train, y_train = extract_1_3_5_digits(mnist_data['train_images'], mnist_data['train_labels'])\n",
    "X_test, y_test = extract_1_3_5_digits(mnist_data['test_images'], mnist_data['test_labels'])\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Add random noise as per Dr Hoover's email suggestion\n",
    "X_train = X_train + 0.01 * np.random.rand(X_train.shape[0], X_train.shape[1])\n",
    "X_test = X_test + 0.01 * np.random.rand(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "# Plot 3x3 grid of first 9 digits\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Digit: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 (30pts). Similar to the above problem A, write a function called LDA that takes the image data matrix (this can be sorted by class to make your life easier) and the number of classes and returns the $C-1$-dimensional change of basis that maximizes:\n",
    "$$\n",
    "    J(\\textbf{w}) = \\frac{\\textbf{w}^T S_b \\textbf{w}}{\\textbf{w}^T S_w \\textbf{w}}\n",
    "$$\n",
    "where $S_w$ is the $\\textbf{within-class}$ scatter matrix and $S_b$ is the $\\textbf{between-class}$ scatter matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Implementation\n",
    "def LDA(x_data, y_data, c_classes):\n",
    "    n_samples, n_features = x_data.shape\n",
    "    unique_classes = np.unique(y_data)\n",
    "    mean_overall = np.mean(x_data, axis=0).reshape(n_features, 1) # Overall mean\n",
    "    \n",
    "    S_b = np.zeros((n_features, n_features)) # Betn class scatter mat\n",
    "    S_w = np.zeros((n_features, n_features)) # Within class scatter mat\n",
    "    \n",
    "    # Get class means and scatter matrices\n",
    "    for c in unique_classes:\n",
    "        X_c = x_data[y_data == c]\n",
    "        n_c = X_c.shape[0]\n",
    "        mean_c = np.mean(X_c, axis=0).reshape(n_features, 1) # Class mean\n",
    "        \n",
    "        # Betn class scatter\n",
    "        mean_diff = mean_c - mean_overall\n",
    "        S_b += n_c * np.dot(mean_diff, mean_diff.T)\n",
    "        \n",
    "        # Within class scatter\n",
    "        for i in range(n_c):\n",
    "            x_i = X_c[i].reshape(n_features, 1)\n",
    "            x_diff = x_i - mean_c\n",
    "            S_w += np.dot(x_diff, x_diff.T)\n",
    "\n",
    "    # Better formula version: inv(S_w) * S_b * w = lambda * w (Using this one)\n",
    "    \n",
    "    epsilon = 1e-10 # Regularize S_w to avoid singularity issues\n",
    "    S_w_reg = S_w + epsilon * np.eye(n_features)\n",
    "    \n",
    "    # Inverse of S_w\n",
    "    try:\n",
    "        S_w_inv = np.linalg.inv(S_w_reg)\n",
    "    except np.linalg.LinAlgError:\n",
    "        S_w_inv = np.linalg.pinv(S_w_reg) # Use pseudo-inverse instead\n",
    "    \n",
    "    # Compute M = inv(S_w) * S_b\n",
    "    M = np.dot(S_w_inv, S_b)\n",
    "    \n",
    "    # Compute eigenvals + eigenvecs of M\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(M)\n",
    "    \n",
    "    # Sort eigenvals and corr eigenvecs (desc)\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # Get top (c_classes - 1) eigenvecs\n",
    "    w = eigenvectors[:, :c_classes-1].real\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 (10pts). To investigate the how well the LDA classifier works, let's use the testing set to determine the classification rates for:\n",
    "\n",
    "* The entire testing set (assuming it has been pruned to digits 1,3 and 5)\n",
    "* The classification rate for each individual digit (this will give you a feel for which representations are easier for the model to classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Performance #\n",
    "print(\"Wait! This may take a while :)\")\n",
    "\n",
    "# Perform LDA\n",
    "n_classes = 3  # Corr to 1 3 5\n",
    "lda_vectors = LDA(X_train, y_train, n_classes)\n",
    "\n",
    "# Project data onto LDA subspace\n",
    "X_train_lda = np.dot(X_train, lda_vectors)\n",
    "X_test_lda = np.dot(X_test, lda_vectors)\n",
    "\n",
    "# Classify using nearest centroid classifier\n",
    "def nearest_centroid_classifier(X_train, y_train, X_test):\n",
    "    # Want centroids for each class\n",
    "    unique_classes = np.unique(y_train)\n",
    "    centroids = np.zeros((len(unique_classes), X_train.shape[1]))\n",
    "    \n",
    "    for i, c in enumerate(unique_classes):\n",
    "        centroids[i] = np.mean(X_train[y_train == c], axis=0)\n",
    "    \n",
    "    y_pred = np.zeros(X_test.shape[0], dtype=int) # Test data class pred\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        distances = np.sqrt(np.sum((centroids - X_test[i])**2, axis=1)) # Euc dist to centroids\n",
    "        y_pred[i] = unique_classes[np.argmin(distances)] # Assign class\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Perform classification on the LDA-projected test data\n",
    "y_pred = nearest_centroid_classifier(X_train_lda, y_train, X_test_lda)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = np.mean(y_pred == y_test) * 100\n",
    "print(f\"\\nOverall Classification Accuracy: {overall_accuracy:.2f}%\\n\")\n",
    "\n",
    "# Calculate accuracy per digit\n",
    "unique_classes = np.unique(y_test)\n",
    "for c in unique_classes:\n",
    "    mask = (y_test == c)\n",
    "    accuracy = np.mean(y_pred[mask] == y_test[mask]) * 100\n",
    "    print(f\"Digit {c} Classification Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem C (10pts)**\n",
    "* In Problem B, you investigated classifying the first three odd digits in the MNIST dataset (digits 1,3, and 5).  Considering that LDA returns at most $C-1$ subspace vectors, we can easily plot these vectors (for this problem) in $\\mathbb{R}^2$ to illustrate how well the class separation is.  Let's compare this plot with our PCA subspace to see which of the two methods gives better class separation (i.e., generate a subplot with the 2-dimensional PCA embedding on the left and the 2-dimensional LDA embedding on the right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA implementation\n",
    "def PCA(X, n_components):\n",
    "    X_centered = X - np.mean(X, axis=0) # Center data\n",
    "    cov_matrix = np.dot(X_centered.T, X_centered) / X.shape[0] # Convariance mat\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix) # Get eigenvals + eigenvecs\n",
    "    \n",
    "    # Sort eigenvals and corr eigenvecs (desc)\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # Select top n_components eigenvecs\n",
    "    components = eigenvectors[:, :n_components]\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Perform PCA with 2 components\n",
    "pca_vectors = PCA(X_train, 2)\n",
    "\n",
    "# Project data onto PCA subspace\n",
    "X_train_pca = np.dot(X_train, pca_vectors)\n",
    "X_test_pca = np.dot(X_test, pca_vectors)\n",
    "\n",
    "# LDA vs PCA projection\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# PCA projection\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['r', 'g', 'b']\n",
    "for i, c in enumerate(unique_classes):\n",
    "    plt.scatter(X_train_pca[y_train == c, 0], X_train_pca[y_train == c, 1], \n",
    "                c=colors[i], label=f'Digit {c}', alpha=0.6)\n",
    "plt.title('PCA Projection')\n",
    "plt.xlabel('First PrincipalComponent')\n",
    "plt.ylabel('Second PrincipalComponent')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# LDA projection\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, c in enumerate(unique_classes):\n",
    "    plt.scatter(X_train_lda[y_train == c, 0], X_train_lda[y_train == c, 1], \n",
    "                c=colors[i], label=f'Digit {c}', alpha=0.6)\n",
    "plt.title('LDA Projection')\n",
    "plt.xlabel('First Discriminant')\n",
    "plt.ylabel('Second Discriminant')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LDA >>>>>> PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
